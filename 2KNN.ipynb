{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2KNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WayneMao/ML/blob/master/2KNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPlLiv9SFwxC",
        "colab_type": "text"
      },
      "source": [
        "### KNN\n",
        "**KNN**: K近邻算法。\n",
        "\n",
        "KNN算法分类问题：\n",
        "- 欧氏距离\n",
        "- 投票模块\n",
        "\n",
        "K取单数防平局\n",
        "\n",
        "总结：\n",
        "1. 一个极其简单算法，适用于低维空间\n",
        "2. KNN在训练过程中是指上不需要做任何事情，所以训练本身不产生任何时间上的消耗\n",
        "3. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtAXmd2jEjls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffbbef86-4bf7-46c7-9da2-791c8a48d8a2"
      },
      "source": [
        "from sklearn import datasets\n",
        "from collections import Counter  # 为了做投票\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# 导入iris数据\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2003)\n",
        "\n",
        "\n",
        "def euc_dis(instance1, instance2):\n",
        "\t\"\"\"\n",
        "\t计算两个样本instance1和instance2之间的欧式距离\n",
        "\tinstance1: 第一个样本， array型\n",
        "\tinstance2: 第二个样本， array型\n",
        "\t\"\"\"\n",
        "\t# TODO\n",
        "\tdist = np.sqrt( np.sum((instance1 - instance2)**2))\n",
        "\treturn dist\n",
        "    \n",
        "    \n",
        "def knn_classify(X, y, testInstance, k):\n",
        "    distances = [euc_dis(x,testInstance) for x in X]\n",
        "    kneighbors = np.argsort(distances)[:k]\n",
        "    count = Counter(y[kneighbors])\n",
        "    return count.most_common()[0][0]\n",
        "\"\"\"\n",
        "\t给定一个测试数据testInstance, 通过KNN算法来预测它的标签。 \n",
        "\tX: 训练数据的特征\n",
        "\ty: 训练数据的标签\n",
        "\ttestInstance: 测试数据，这里假定一个测试数据 array型\n",
        "\tk: 选择多少个neighbors? \n",
        "\t\"\"\"\n",
        "\t# TODO  返回testInstance的预测标签 = {0,1,2} \n",
        "\n",
        "    \n",
        "    \n",
        "# 预测结果。    \n",
        "predictions = [knn_classify(X_train, y_train, data, 3) for data in X_test]\n",
        "correct = np.count_nonzero((predictions==y_test)==True)\n",
        "print (\"Accuracy is: %.3f\" %(correct/len(X_test)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy is: 0.921\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuwQqiz8jd4V",
        "colab_type": "text"
      },
      "source": [
        "### 交叉验证\n",
        "\n",
        "交叉验证是机器学习建模中非常非常重要的一步，也是大多数人所说的“调参”的过程。  \n",
        "**核心思想：**把一些可能的K逐个去尝试一遍，然后选出效果最好的K值。  \n",
        "**K折交叉验证(K-fold Cross Validation):** 我们先把训练数据再分成训练集和验证集，之后使用训练集来训练模型，然后再验证集上评估模型的准确率。  \n",
        "针对不同的K值，逐一尝试从而选择最好的。并且，这种参数我们称作**超参数(Hyperparameter).**  \n",
        "**留一法交叉验证**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfwuTbOjjNFB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# KNN交叉验证\n",
        "\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import KFold # 主要用于K折交叉验证\n",
        "\n",
        "#以下导入iris数据集\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "print(X.shape,y.shape) # (150, 4) (150,)\n",
        "\n",
        "# 定义我们想要搜索的K值(候选集)，这里定义8个不同的值\n",
        "ks = [1,3,5,7,9,11]\n",
        "\n",
        "# 进行5折交叉验证， KFold返回的是每一折中训练数据和验证数据的index\n",
        "# 假设数据样本为：[1,3,5,6,11,12,43,12,44,2],总共十个样本\n",
        "# 则返回的kf的格式为（前面的是训练数据，后面的是验证集\n",
        "# [0,1,3,5,6,7,8,9],[2,4]\n",
        "kf = KFold(n_splits=5, random_state= 2001, shuffle=True) # shuffle：在每次划分时，是否进行洗牌\n",
        "\n",
        "# 保存当前最好的K值和对应的准确率值\n",
        "best_k = ks[0]\n",
        "best_score = 0\n",
        "\n",
        "# 循环每一个K值\n",
        "for k in ks:\n",
        "  curr_score = 0\n",
        "  for train_index, valid_index in kf.split(X):\n",
        "    # 每一折的训练及计算准确率\n",
        "    clf = KNeighborsClassifier(n_neighbors=k)# init\n",
        "    clf.fit(X[train_index],y[train_index])\n",
        "    curr_score = curr_score + clf.score(X[valid_index],y[valid_index]) #正确率打分\n",
        "\n",
        "    #求一下5折的平均准确率\n",
        "    avg_score = curr_score/5\n",
        "    if avg_score > best_score:\n",
        "      best_k = k\n",
        "      best_score = avg_score\n",
        "    print(\"Current best score is %0.2f\"%best_score, \"best k: %d\"%best_k)\n",
        "\n",
        "print(\"After cross validation, the final best k is: %d\"%best_k)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpHw2G2rmv3d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e638c2db-9181-454d-cd04-b26dcace2619"
      },
      "source": [
        "# KNN交叉验证\n",
        "from sklearn.model_selection import GridSearchCV # 通过网格方式来搜索参数\n",
        "# 自动调参， 适用于小数据集\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "#以下导入iris数据集\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 设置需要搜索的K值， n_neighbors 是sklearn中KNN的参数\n",
        "parameters = {'n_neighbors':[1,3,5,7,9,11,13,15]}\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "# 通过GridSearchCV来搜索最好的K值， 这个模块内部==对每个K值做了评估\n",
        "\n",
        "clf = GridSearchCV(knn,parameters,cv=5)\n",
        "clf.fit(X,y)\n",
        "\n",
        "# 输出最好的参数以及对应的准确率\n",
        "print(\"Best score is:%.2f\"%clf.best_score_, \"Best param:\",clf.best_params_)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best score is:0.98 Best param: {'n_neighbors': 7}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Evvn4Y-Lx-1a",
        "colab_type": "text"
      },
      "source": [
        "### 特征缩放\n",
        "#### 1. 线性归一化(Min-max Normalization)\n",
        "\n",
        "$$ X_{new} = \\frac {X - min(X)} {max(X) - min(X)}  $$\n",
        "\n",
        "####2. 标准差归一化(Z-score Normalization)\n",
        "$$ X_{new} = \\frac {X - mean(X)} {std(X)} $$\n",
        " \n",
        " $std(X)$ 标准差\n",
        " 特征映射到 N~(0,1） 正态分布区间"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdaDXCEuxDiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}