{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14 提升树：许多“学渣”共同决策\n",
    "\n",
    "### 14.1 提升树的核心思想\n",
    "Bagging是很多过拟合的weak leaner来组成，Boosting是很多欠拟合的weak learner来组成，这就是它们之间核心的区别。   \n",
    "\n",
    "上面这句话或许有点难理解，可以举个例子：Bagging模型可以理解成由很多顶级的专家来组成，但这些专家呢，都自以为很厉害都听不进去别人的话，所以遇到新的问题适应能力稍微弱一些。但是呢，让这些专家通过合作来做事情的时候就非常厉害。\n",
    "\n",
    "另外一方面，Boosting模型可以理解成由很多学渣来组成，每一个人的能力都挺弱的，而且不能够独当一面。但是呢，当很多这批人一起合作的时候却能带来很厉害的结果。\n",
    "\n",
    "Boosting方法很多时候比Bagging还要厉害！而且在过拟合问题的解决上，Boosting也被证明出更加有效。  \n",
    "可以搜索 \"generalization bound\", \"PAC learning\"等关键词来了解一下Boosting在这块的一些理论工作。\n",
    "\n",
    "最初提出来的Boosting方法叫作Adaboost，目前用得不多。但它的思想一步步被学者们利用被改进，最后衍生出了很多流行的模型，比如**GBDT、XGBoost**等模型。\n",
    "特别是最近几年，很多工业级应用中都倾向于使用Boosting，也证实了它在实际问题中的有效性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2 基于残差的训练方式\n",
    "**基于残差(residual)的训练**方式跟Bagging的训练是有很大区别的。Bagging的训练可以理解为并行的训练，其实每一个模型训练时都互不干涉的。但基于残差的Boosting模型的训练是串行的。 \n",
    "\n",
    "总体来讲，就是模型1还没有解决的部分(残差)，让模型2去解决，模型2还没有解决的再由模型3来解决，以此类推…  \n",
    "**最终预测** = 模型1的预测 + 模型2的预测 + 模型3的预测 + ···\n",
    "\n",
    "一个求平均，一个求和，这是跟Bagging的本质区别。  \n",
    "Boosting里最为流行的模型称作XGBoost。 具体论文地址请参考：https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf  \n",
    "\n",
    "**XGBoosting**是工业界、kaggle比赛大杀器。学习路径如下：  \n",
    "> - 如何构造目标函数？  \n",
    "- 目标函数直接优化难，如何近似？\n",
    "- 如何把树的结构引入到目标函数？\n",
    "- 仍然难优化，考虑使用贪心算法？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3 XGBoost的目标函数构建\n",
    "XGBoost的目标函数由两个部分来构成，其中一项是**损失函数**，另外一项是控制模型的复杂度**(正则项)**。    \n",
    "如果是回归问题，我们可以使用**最小二乘**作为损失函数；如果是分类问题，我们则可以使用**交叉熵**作为损失函数；这里的函数l(.,.)l(.,.)表示预测值与真实值之间的差异，可以灵活地做出选择。  \n",
    "\n",
    "在学习决策树的时候已经谈论过几条控制过拟合的手段。**一棵树越完整意味着这棵树越容易过拟合。**  \n",
    "让这棵树容易过拟合的，无非跟几个经典的特性相关，比如一个树的深度越深就越容易过拟合； 一个树越深也意味着它的叶节点越多，所以叶节点个数也可以作为衡量复杂度的一个很重要的要素。  \n",
    "除了这几个方面，在回归问题上，叶节点对应的预测值也是可以用来控制模型的复杂度的。比如这个预测值越大就说明模型越复杂，预测值越小说明模型越简单。  \n",
    "那最后一点又如何去理解呢？ 比如我们做回归分析，最终的预测结果是所有决策树预测结果之和。所以从这个角度可以这么想，如果一棵决策树的预测是比较少，那相当于这棵树带来的影响就会越小，当然这也说明我们需要更多的树。  \n",
    "\n",
    "总结起来，树的复杂度可以由几个方面来决定：\n",
    "- 叶节点的个数\n",
    "- 树的深度\n",
    "- 叶节点值\n",
    "- ···\n",
    "\n",
    "\n",
    "Reference:   \n",
    "**推荐**：[通俗理解kaggle比赛大杀器xgboost](https://blog.csdn.net/v_july_v/article/details/81410574)  \n",
    "**bilibili:**[机器学习算法:XGBoost](https://www.bilibili.com/video/av50905316/?p=3)  \n",
    "XGBoost具体论文地址请参考：https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf    \n",
    "XGBoost学习最好的资源无非是Tianqi自己写的PPT，链接请参考：https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4 泰勒展开式与目标函数的近似\n",
    "通过泰勒展开式我们可以近似一个函数到一阶、二阶或者更高阶。  \n",
    "之前涉及的梯度下降法也可以从泰勒展开式做进一步的分析、或者牛顿法也是跟泰勒展开式有紧密的关系。  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
