{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 正则regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 过拟合\n",
    "如果假定给定的数据是线性可分的，这时候使用逻辑回归模型会发生什么情况？\n",
    "- 这时候，学出来的参数趋向于无穷大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**正则** \n",
    "$J = J_0 + \\lambda \\sum||W||^2 $  \n",
    "假设$L=\\lambda \\sum||W||^2$，那么我们就可以把任务看做是**在L的约束条件下，$J_0$取最小值的时候的解** \n",
    "\n",
    "正则为什么能防止过拟合呢？  \n",
    "正则能帮助我们剔除掉复杂的模型，留下简单的模型，简单模型不容易过拟合。（**正则缩小可行解空间**）\n",
    "\n",
    "正则是一个很大的领域，它既可以用在逻辑回归，也可以用在SVM、线性回归、神经网络、各类深度学习模型里。而且正则是一种能够**把先验知识加入到模型里的最直接的方式。**\n",
    "\n",
    "Reference：  \n",
    "[机器学习中正则化项L1和L2的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 模型的泛化以及过拟合\n",
    "构建泛化能力强的模型需要：\n",
    "\n",
    "第一、需要正确的数据。我们不能期待使用一个错误的数据来构建一个泛化能力强的模型。比如数据里包含了大量的噪声，这很难让我们训练出有效模型出来。\n",
    "\n",
    "\n",
    "第二、需要选择合适的模型。比如图像识别，我们都知道CNN是最合适的模型； 对于构建评分卡，可能集成模型是比较合适的。这里没有一个绝对的理论，很多都是通过不断尝试得到的经验。\n",
    "\n",
    "\n",
    "第三、需要选择合适的优化算法。针对于复杂的模型，这一点尤其重要。比如面对复杂的深度学习模型，我们可以选择梯度下降法，也可以选择Adam等算法。但每一种不同类型的优化算法给我们带来不同质量的解，而且这种解带来的泛化能力也是不一样的。\n",
    "\n",
    "\n",
    "第四、需要避免过拟合现象。在前面所讲的条件不变的情况下，过拟合是最核心的问题。我们需要通过一些手段来避免或者降低过拟合现象。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 L1与L2正则\n",
    "\n",
    "| 名称       | L2   | L1   |\n",
    "| :----------: | :-------: | :------: |\n",
    "| 数学表达式 |  式(7-1)    |  式(7-2)    |\n",
    "| 作用       | 1.让参数变小   | 1.让参数变小 2.稀疏性：让很多参数为0 |     |\n",
    "\n",
    "$||w||^2_2 = w^2_1 + w^2_2 + \\dots +w^2_D$ (7-1)  \n",
    "$||w||_1 = |w_1| + |w_2| + \\dots + |w_D|$ (7-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面用代码来验证一下L1和L2的范数对稀疏性的作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(L1)逻辑回归的参数为：\n",
      " [[ 0.          0.          0.00424842  0.          0.07096305 -0.29531785\n",
      "   0.         -0.3403994   0.          0.78414305  0.          0.10093459\n",
      "   0.          0.          0.         -0.04089472  0.          0.\n",
      "   0.41568396  0.        ]]\n",
      "(L2)逻辑回归的参数为：\n",
      " [[-0.06020774 -0.08587293  0.06269959  0.0218838   0.36622515 -0.45899841\n",
      "   0.11456309 -0.44218794 -0.24780618  0.87767764 -0.32403048  0.27800343\n",
      "   0.34313572  0.16393398 -0.14322159 -0.22759078  0.09331433 -0.22950935\n",
      "   0.48553032  0.1213868 ]]\n"
     ]
    }
   ],
   "source": [
    "# 随机生成样本数据。二分类问题，每一个类别生成5000个样本数据\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 100 # 生成正负样本各100个\n",
    "\n",
    "# 利用高斯分布来生成样本，首先需要生成 convariance matrix\n",
    "# 由于假设我们生成20维的特征向量，所以矩阵大小为 20*20\n",
    "rand_m = np.random.rand(20,20)\n",
    "# 保证矩阵为PSD矩阵（半正定）\n",
    "cov = np.matmul(rand_m.T,rand_m)\n",
    "\n",
    "# 通过高斯分布生成样本\n",
    "x1 = np.random.multivariate_normal(np.random.rand(20),cov,num_observations)\n",
    "x2 = np.random.multivariate_normal(np.random.rand(20)+5,cov,num_observations)\n",
    "\n",
    "X = np.vstack((x1,x2)).astype(np.float32)\n",
    "y = np.hstack((np.zeros(num_observations),np.ones(num_observations)))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 使用L1的正则，C为控制正则的参数。C值越大，正则项的强度会越弱。\n",
    "clf = LogisticRegression(fit_intercept=True, C=0.1, penalty='l1', solver='liblinear')\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(\"(L1)逻辑回归的参数为：\\n\",clf.coef_)\n",
    "\n",
    "# 使用L2的正则，C为控制正则的参数。C值越大，正则项的强度会越弱。\n",
    "clf = LogisticRegression(fit_intercept=True, C=0.1, penalty='l2', solver='liblinear')\n",
    "clf.fit(X,y)\n",
    "\n",
    "print(\"(L2)逻辑回归的参数为：\\n\",clf.coef_)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1缺点:** L1范数在0点不具备梯度，所以需要做一些特殊处理，比如使用subgradient来代替梯度。L1范数虽然有特征选择的功能，但也有一些不足。比如多个特征具有强相关性，那通过L1正则选出来的特征可能是这些特征里的任意个，但实际上特征还是有好有坏。即便特征之间有强相关性，但肯定其中有最好的特征。\n",
    "\n",
    "为了弥补这个缺点，学者们提出了一个方案，就是联合L1和L2正则一起使用。 如果我们在线性回归的基础上联合使用了此两个正则，这个模型就是非常著名的ElasticNet。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 正则与最大后验概率\n",
    "最大似然本身是不可能把先验知识考虑进去的。  \n",
    "那如何才能做到把先验知识考虑进去呢？ 答案就是使用最大后验估计, 简称MAP！\n",
    "\n",
    "对于最大似然估计，我们需要最大化的是p(D|\\theta)p(D∣θ)， 然而在最大后验估计里，我们需要最大化的是p(\\theta|D)p(θ∣D)，这也叫做后验概率。 所以也把它叫作最大后验估计。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
