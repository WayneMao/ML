{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 新闻推荐项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此项目的目的是搭建一个简易的新闻推荐性, 采用的数据集为英国BBC新闻。通过此项目，你可以学到如何使用主题模型在推荐系统当中。需要经历的几个步骤是：\n",
    "1. 文本数据的预处理\n",
    "2. 通过新闻的tf-idf词率，并使用主题模型来给每一个新闻抽取主题的分布\n",
    "3. 通过用户历史新闻的浏览记录，并建立用户关注的主题分布，并为用户做推荐。\n",
    "\n",
    "需要安装的工具： gensim\n",
    "在本想中，其实你需要完成的部分不多，主要希望大家可以仔细阅读一下代码，并试图去理解其中的一些细节。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-688375c3912d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnowball\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSnowballStemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# 确保这些环境已经OK! \n",
    "\n",
    "from gensim import corpora, models, parsing\n",
    "from glob import glob\n",
    "import warnings\n",
    "import os,sys\n",
    "import re,string\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 设置模型训练时用到的参数 （你可以改改并看一下改动后的效果）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 只考虑新闻长度大于如下给定值的情况\n",
    "min_length = 200\n",
    "# 主题数量, 超参, 需要根据情况调整，代表的是针对于每一个新闻要提取多少个主题。 \n",
    "num_topics = 90\n",
    "# 设置一个单词出现在数据集中的最小次数, 小于这个次数的单词会不予考虑, \n",
    "# 这是为了减小总的单词数, 出现次数太小的单词对主题的贡献可以忽略不计\n",
    "no_below_this_number = 50\n",
    "# 设置一个单词出现在数据集中的最大次数, 如果单词出现在文章中的次数大\n",
    "# 于文章总数乘以如下百分比, 则不予考虑. 这是因为太频繁出现在多个文章\n",
    "# 中的单词很可能为Stop_Words, 例如is, am, of, at等, 它们对主题\n",
    "# 的贡献也可以忽略不计\n",
    "no_above_fraction_of_doc = 0.2\n",
    "# 如果一个主题的权重低于如下值, 则将其从主题中去除, 说明这个主题很\n",
    "# 不重要, 需要按照实际情况调整。\n",
    "remove_topic_so_less = 0.05\n",
    "# 推断语料库的主题分布时，最大迭代次数。数据集越小, 需要迭代次数\n",
    "# 越大, 需要按照实际情况调整。\n",
    "num_of_iteration = 1000\n",
    "# 训练的迭代次数, 即遍历数据集的次数, 需要按照实际情况调整\n",
    "passes = 3\n",
    "# stop_words 停用词\n",
    "stopword = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 数据的处理： 对数据集进行tokenization\n",
    "``请完成TODO 部分，大部分的处理过程已经帮你写完``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tokenzing the corpora: BBCNews\n",
      "Have processed 500 documents\n",
      "Have processed 1000 documents\n",
      "Have processed 1500 documents\n",
      "Have processed 2000 documents\n",
      "Have processed 2500 documents\n",
      "Have processed 3000 documents\n",
      "Have processed 3500 documents\n",
      "Have processed 4000 documents\n",
      "Have processed 4500 documents\n",
      "Have processed 5000 documents\n",
      "Have processed 5500 documents\n"
     ]
    }
   ],
   "source": [
    "pathToCorpora = \"BBCNews\"\n",
    "print('Start tokenzing the corpora: %s' % (pathToCorpora))\n",
    "punct = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "wnl = WordNetLemmatizer()  # 使用基于worldnet的lemmzation操作\n",
    "\n",
    "# 新闻的序号, 从0开始\n",
    "doc_count=0\n",
    "train_set = []\n",
    "doc_mapping = {}\n",
    "link_mapping = {}\n",
    "\n",
    "for f in glob(pathToCorpora+'/*'):\n",
    "    filereader = open(f, 'r')\n",
    "    article = filereader.readlines();\n",
    "    filereader.close()\n",
    "    text = ''\n",
    "    try: # 抽取新闻中的链接，标题以及文本\n",
    "        link = article[0]\n",
    "        title = article[1]\n",
    "        text = article[2].lower()\n",
    "    except IndexError:\n",
    "        continue\n",
    "\n",
    "    # 如果新闻长度太小，不考虑\n",
    "    if len(text) < min_length:\n",
    "        continue\n",
    "    \n",
    "    # 去掉新闻中的标点符号\n",
    "    text = punct.sub(\"\",text)  \n",
    "    \n",
    "    # 将每一条新闻转为单词集合\n",
    "    tokens = nltk.word_tokenize(text)  \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO： 从单词中去除 stopword, 然后对单词进行Lemmatize (例如将单词 go, goes, going, gone 都转为 go)\n",
    "    # 并添加到训练数据集train_set中\n",
    "    # your code ...\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 将文章的标题加入doc_mapping字典, 使用文章出现的序号为字典的key\n",
    "    doc_mapping[doc_count] = title\n",
    "    # 将文章的http链接加入link_mapping字典, 使用文章出现的序号为字典的key\n",
    "    link_mapping[doc_count] = link\n",
    "    # 为新闻的序号添加 1 \n",
    "    doc_count = doc_count+1\n",
    "    # 每处理 500 篇文献, 打印处理的新闻数量\n",
    "    if doc_count % 500 == 0:\n",
    "        print( 'Have processed %i documents' % (doc_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据集一共包含 5863 条新闻\n",
      "原来的字典中一共包含  0  个单词\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-30d1367f5d94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'数据集一共包含 %i 条新闻'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdoc_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"原来的字典中一共包含 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenominator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" 个单词\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m\"去除频次异常的单词后, 一共有 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnominator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" 个单词, 减少的百分比为 \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnominator\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdenominator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'完成数据的清洗'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# 将训练数据放入 gensim.corpora 的字典中, 为后续的训练做准备\n",
    "dic = corpora.Dictionary(train_set)\n",
    "\n",
    "# 从字典中去除出现频次太多或者太少的单词\n",
    "denominator = len(dic)\n",
    "dic.filter_extremes(no_below=no_below_this_number, no_above=no_above_fraction_of_doc)\n",
    "nominator = len(dic)\n",
    "\n",
    "# 将训练数据中的单词转换为词袋(bag of words)的方式\n",
    "corpus = [dic.doc2bow(text) for text in train_set]\n",
    "print( '数据集一共包含 %i 条新闻' % (doc_count))\n",
    "print( \"原来的字典中一共包含 \", denominator, \" 个单词\")\n",
    "print( \"去除频次异常的单词后, 一共有 \", nominator, \" 个单词, 减少的百分比为 \", (1-(nominator/denominator)),\"%\")\n",
    "print( '完成数据的清洗')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 训练主题模型\n",
    "``完成主题模型训练部分，已经标注为TODO``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将训练数据中的单词从词袋(bag of words)的表达方式转为Tfidf(Term Frequency, Inverse Document Frequency)的方式\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "\n",
    "\n",
    "# TODO: 训练主题模型, 调用合适的函数，并传入参数，试图了解一下参数的含义\n",
    "lda = \n",
    "corpus_lda = lda[corpus_tfidf]\n",
    "\n",
    "\n",
    "\n",
    "# 训练结束以后, 打印所有的主题及及其相关最频繁的单词\n",
    "for i in range(num_topics):\n",
    "    print( '主题 %s : ' % (str(i)) + lda.print_topic(i))\n",
    "\n",
    "# 计算当前主题模型对应的 perplexity. 其值越小说明主题模型越好\n",
    "print( '===============================')\n",
    "print( '主题模型的 perplexity : ',lda.bound(corpus_lda),' 主题数量为 k =', str(num_topics))\n",
    "print( '===============================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 保存主题模型及相关数据, 这部分代码已经给你写完，供参考。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义辅助函数, 将列表转为字典到\n",
    "def __convertListToDict(anylist):\n",
    "    convertedDict = {}\n",
    "    for pair in anylist:\n",
    "        topic = pair[0]\n",
    "        weight = pair[1]\n",
    "        convertedDict[topic] = weight\n",
    "    return convertedDict\n",
    "\n",
    "# 保存python对象到文件中\n",
    "def __savePickleFile(fileName,objectName):\n",
    "    fileName= './LDAmodel/'+fileName+'.pickle'\n",
    "    mappingFile = open(fileName,'wb')\n",
    "    pickle.dump(objectName,mappingFile)\n",
    "    mappingFile.close()\n",
    "    \n",
    "\n",
    "# 保存LDA模型 \n",
    "save_path = './LDAmodel/final_ldamodel'\n",
    "lda.save(save_path)\n",
    "\n",
    "# 保存数据集\n",
    "save_path = 'corpus'\n",
    "__savePickleFile(save_path,corpus)\n",
    "\n",
    "# 保存序号到文章标题的字典\n",
    "save_path = 'documentmapping'\n",
    "__savePickleFile(save_path,doc_mapping)\n",
    "\n",
    "# 保存序号到文章链接的字典\n",
    "save_path = 'linkmapping'\n",
    "__savePickleFile(save_path,link_mapping)\n",
    "\n",
    "# 保存文章序号到主题分布的字典\n",
    "doc_topic_matrix = {}\n",
    "count = 0\n",
    "for doc in corpus:\n",
    "    dense_vector = {}\n",
    "    vector = __convertListToDict(lda[doc])\n",
    "    # 去除权重太低的主题\n",
    "    for topic in vector:\n",
    "        if vector[topic] > remove_topic_so_less:\n",
    "              dense_vector[topic] = vector[topic]\n",
    "    doc_topic_matrix[count] = dense_vector\n",
    "    count = count+1\n",
    "save_path = 'doc_topic_matrix'\n",
    "__savePickleFile(save_path,doc_topic_matrix)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 使用主题模型做推荐\n",
    "\n",
    "#### 假设摸一个用户历史上阅读过的问题序号为 10, 21, 31, 我们需要为此用户推荐新闻。 请认真此段代码，并试图理解一下。 代码的最后部分需要你完成。 \n",
    "``TODO部分需要完成``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从保存的模型文件中加载模型\n",
    "path_doc_topic_matrix = './LDAmodel/doc_topic_matrix.pickle'\n",
    "mappingFile = open(path_doc_topic_matrix,'rb')\n",
    "doc_topic_matrix = pickle.load(mappingFile)\n",
    "mappingFile.close()\n",
    "\n",
    "# 加载文章序号到新闻标题的字典\n",
    "path_mappingfile= './LDAmodel/documentmapping.pickle'\n",
    "mappingFile = open(path_mappingfile,'rb')\n",
    "mapping = pickle.load(mappingFile)\n",
    "mappingFile.close()\n",
    "\n",
    "# 加载文章序号到新闻链接的字典\n",
    "path_mappingfile= './LDAmodel/linkMapping.pickle'\n",
    "mappingFile = open(path_mappingfile,'rb')\n",
    "linkMapping = pickle.load(mappingFile)\n",
    "mappingFile.close()\n",
    "        \n",
    "# 假设摸一个用户历史上阅读过的新闻序号为 10, 21, 31, 我们为此用户计算其新闻喜好的主题模型\n",
    "# 建立的方式是将其阅读过的新闻的主题的权重相加并求均值\n",
    "user_dict = {}\n",
    "for i in [10, 21, 31]:\n",
    "    user_dict[i] = doc_topic_matrix[i]\n",
    "\n",
    "omit_topic_below_this_fraction = 0.1\n",
    "user_topic_vector = {}\n",
    "length = len(user_dict)\n",
    "for seen_doc in user_dict:\n",
    "    for seen_topic in user_dict[seen_doc]:\n",
    "        weight = user_dict[seen_doc][seen_topic]\n",
    "        if seen_topic in user_topic_vector:\n",
    "            current_weight = user_topic_vector[seen_topic]\n",
    "            current_weight = current_weight + weight/length\n",
    "            user_topic_vector[seen_topic] = current_weight\n",
    "        else:\n",
    "            user_topic_vector[seen_topic] = weight/length\n",
    "        \n",
    "# 去除权重低于 omit_topic_below_this_fraction/2 的主题\n",
    "normalized_user_topic_vector = {}\n",
    "for k,v in user_topic_vector.items():\n",
    "    if v > omit_topic_below_this_fraction/2:\n",
    "        normalized_user_topic_vector[k] = v\n",
    "    \n",
    "# TODO 对主题分布做归一化, 使得所有权重之和为 1 \n",
    "# 最后的输出为： 当前用户感兴趣的新闻主题分布 : {5: 0.6319591628063931, 11: 0.2349756591871135, 80: 0.1330651780064935}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print( '当前用户感兴趣的新闻主题分布 : {0}'.format(normalized_user_topic_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于用户感兴趣的主题分布，给他推荐类似的新闻，这部分需要计算相似度。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Pearson correlation 计算每一个文章的主题分布和用户感兴趣的主题分布的相似性\n",
    "def fill_list_from_dict(a,topics):\n",
    "    result = [0] * topics\n",
    "    for k,v in a.items():\n",
    "        result[k-1] = v\n",
    "    return result\n",
    "    \n",
    "def pearson_correlation(a,b,topics):\n",
    "    from scipy.stats import pearsonr\n",
    "    a = fill_list_from_dict(a,topics)\n",
    "    b = fill_list_from_dict(b,topics)\n",
    "    return pearsonr(a,b)[0]  \n",
    "\n",
    "recommend_dict={}\n",
    "for doc in doc_topic_matrix:\n",
    "    sim = pearson_correlation(normalized_user_topic_vector, doc_topic_matrix[doc], lda.num_topics)\n",
    "    if doc not in user_dict.keys():  \n",
    "        recommend_dict[doc] = sim\n",
    "\n",
    "# 为用户推荐的新闻数量\n",
    "def getOrderedDict(dic):\n",
    "    from operator import itemgetter\n",
    "    from collections import OrderedDict\n",
    "    sorteddict = OrderedDict(sorted(dic.items(), skey=itemgetter(1),reverse=True))\n",
    "    return sorteddict\n",
    "\n",
    "no_of_recommendation = 5\n",
    "sort = getOrderedDict(recommend_dict)\n",
    "recommend_str = (str(list(sort.keys())[:no_of_recommendation])\n",
    "                .replace('[','')\n",
    "                .replace(']','')\n",
    "                )\n",
    "\n",
    "print('===============用户读过的新闻===========')\n",
    "for i in user_dict:\n",
    "    print('id: {0}, 标题: {1}, 链接: {2}'.format(i, mapping[i], linkMapping[i]))\n",
    "\n",
    "print(\"===============为用户推荐的新闻=========\")\n",
    "for i in list(sort.keys())[:no_of_recommendation]:\n",
    "    print('id: {0}, 标题: {1}, 链接: {2}'.format(i, mapping[i], linkMapping[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
